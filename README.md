## üï∑ Structura Crawler

–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫—Ä–∞—É–ª–µ—Ä –Ω–∞ –±–∞–∑–µ Scrapy –¥–ª—è –æ–±—Ö–æ–¥–∞ —Å–∞–π—Ç–æ–≤ –∏ —Å–±–æ—Ä–∞ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –≥–ª—É–±–∏–Ω–µ, –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç—Ä–∞–Ω–∏—Ü, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π –∏ —ç–∫—Å–ø–æ—Ä—Ç –≤ JSON.

---

### üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞

```bash
git clone https://github.com/igralkin/StructuraScrapper.git
cd StructuraScrapper
pip install -r requirements.txt
```

---

### ‚ñ∂Ô∏è –ó–∞–ø—É—Å–∫

```bash
python main.py https://example.com \
  --max-pages 1000 \
  --depth 5 \
  --save-html \
  --output output/data.json
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `start_url` ‚Äî —Å—Ç–∞—Ä—Ç–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
- `--max-pages` ‚Äî –º–∞–∫—Å–∏–º—É–º —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1000)
- `--depth` ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5)
- `--save-html` ‚Äî —Å–æ—Ö—Ä–∞–Ω—è—Ç—å HTML –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ `output/`
- `--output` ‚Äî –ø—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

---

### üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
```
crawler/
‚îú‚îÄ‚îÄ cms_detector/              # –ú–æ–¥—É–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è CMS
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–∫–µ—Ç–∞
‚îÇ   ‚îú‚îÄ‚îÄ detector.py            # –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è CMS –ø–æ URL/HTML
‚îÇ   ‚îî‚îÄ‚îÄ signatures.py          # –°–∏–≥–Ω–∞—Ç—É—Ä—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö CMS
‚îÇ
‚îú‚îÄ‚îÄ main.py                    # CLI-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îú‚îÄ‚îÄ spiders/
‚îÇ   ‚îî‚îÄ‚îÄ base_spider.py         # –õ–æ–≥–∏–∫–∞ –∫—Ä–∞—É–ª–µ—Ä–∞
‚îú‚îÄ‚îÄ settings.py                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Scrapy
‚îú‚îÄ‚îÄ requirements.txt           # –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ Dockerfile                 # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è
‚îú‚îÄ‚îÄ .gitignore                 # –ò—Å–∫–ª—é—á–µ–Ω–∏—è
‚îî‚îÄ‚îÄ output/                    # –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (html, json)
```
---

### üîå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è CMS-–¥–µ—Ç–µ–∫—Ç–æ—Ä–∞

–ú–æ–¥—É–ª—å `cms_detector` –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç CMS –ø–æ HTML-–∫–æ–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã. 

–û–Ω —É–∂–µ –≤—Å—Ç—Ä–æ–µ–Ω –≤ `base_spider.py` –∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç CMS **—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤—ã—Ö 5 HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö** –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

**–§–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ `output/data.json`:**
```json
{
  "url": "https://example.com",
  "depth": 0,
  "cms": "wordpress"
}
```

–í—ã –º–æ–∂–µ—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä `self.pages_crawled <= 5` –≤ `BaseSpider.parse()`.

---

### üê≥ –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Docker

```bash
docker build -t structura-crawler .
docker run structura-crawler https://example.com --max-pages 200 --depth 3 --save-html
```

---

### ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ (settings.py)
- `SKIP_EXTENSIONS` ‚Äî —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å–∫–∞—á–∏–≤–∞—é—Ç—Å—è (–Ω–∞–ø—Ä–∏–º–µ—Ä .pdf, .svg)
- `DEPTH_LIMIT`, `CLOSESPIDER_PAGECOUNT` ‚Äî –ª–∏–º–∏—Ç—ã –∫—Ä–∞—É–ª–µ—Ä–∞

---

### ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞
- –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ (–≤ —Ä–∞–º–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞)
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –æ–±—Ö–æ–¥
- –£–≤–∞–∂–∞–µ—Ç robots.txt
- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è–º
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –±–æ–ª—å—à–∏–µ —Å–∞–π—Ç—ã (1000+ —Å—Ç—Ä–∞–Ω–∏—Ü)

---

### üìå –ü—Ä–∏–º–µ—Ä—ã —Å–∞–π—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
- https://botcreators.ru
- https://structura.app
- https://automatisation.art
- https://mindbox.ru
- https://skillfactory.ru